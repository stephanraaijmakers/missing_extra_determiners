{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6esSCqVDf5rL"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM , AutoTokenizer, pipeline\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import re\n",
        "from huggingface_hub import InferenceClient\n",
        "from math import log2\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "yRUs9PDBpVp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate"
      ],
      "metadata": {
        "id": "EqeFY2lfoKFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LMHeadModel:\n",
        "\n",
        "    def __init__(self, model_name):\n",
        "        # Initialize the model and the tokenizer.\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def get_tokenizer(self):\n",
        "        return self.tokenizer\n",
        "\n",
        "    def get_predictions(self, sentence):\n",
        "        # Encode the sentence using the tokenizer and return the model predictions.\n",
        "        inputs = self.tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(inputs)\n",
        "            predictions = outputs[0]\n",
        "        return predictions\n",
        "\n",
        "    def get_next_word_probabilities(self, sentence, top_k_words):\n",
        "        predictions = self.get_predictions(sentence)\n",
        "        next_token_candidates_tensor =  predictions[0, -1, :]\n",
        "        topk_candidates_indexes = torch.topk(\n",
        "            next_token_candidates_tensor, top_k_words).indices.tolist()\n",
        "        topk_candidates_tokens = \\\n",
        "            [self.tokenizer.decode([idx]).strip() for idx in topk_candidates_indexes]\n",
        "        topk_candidates_indexes=[idx for idx in topk_candidates_indexes]\n",
        "        all_candidates_probabilities = torch.nn.functional.softmax(\n",
        "            next_token_candidates_tensor, dim=-1)\n",
        "        topk_candidates_probabilities = \\\n",
        "            all_candidates_probabilities[topk_candidates_indexes].tolist()\n",
        "\n",
        "        return zip(topk_candidates_tokens, topk_candidates_probabilities)"
      ],
      "metadata": {
        "id": "uSsfvrULgMXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DETERMINERS=[\"the\",\"a\"]\n",
        "DETERMINER_TAGS=['determiner','article']"
      ],
      "metadata": {
        "id": "58TPp_C8gSrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_probs(lines, lmm, top_k, output_file):\n",
        "    print(\"Generating probabilities...\")\n",
        "    Sentences=[]\n",
        "    Outputs=[]\n",
        "    Probs=[]\n",
        "\n",
        "    #HF_TOKEN=os.environ[\"HF_TOKEN\"] # run first: export HUGGINGFACE_TOKEN=\"...\" in shell\n",
        "\n",
        "\n",
        "    HF_TOKEN=userdata.get('HF_TOKEN')\n",
        "\n",
        "    client = InferenceClient(\n",
        "                api_key=HF_TOKEN, # secret key via Colab\n",
        "            )\n",
        "    for i in tqdm(range(len(lines))):\n",
        "        line=lines[i]\n",
        "        sentence=\"\"\n",
        "        full_sent=' '.join(lines[i])\n",
        "\n",
        "        messages = [\n",
        "\t                {\n",
        "\t\t            \"role\": \"user\",\n",
        "\t\t            \"content\": \"Assign parts of speech to the following text. You will output a tag for every separate word. The tags are limited to adjective, noun, verb, preposition, article, adverb, conjunction, propernoun. Output format is: word/tag, per separate word. No asterisk or numbers. Replace spaces in tags with underscores. Example: I ate bananas. Output: I/pronoun ate/verb bananas/noun. Sentence: \"+full_sent\n",
        "\t                }\n",
        "                    ]\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "                 model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "\t             messages=messages,\n",
        "\t             max_tokens=500,\n",
        "                 temperature=0.0,\n",
        "            )\n",
        "\n",
        "        pos_tagged=completion.choices[0].message.content\n",
        "        pos_tagged=re.sub(\"\\n\",\" \",pos_tagged)\n",
        "        #print(pos_tagged)\n",
        "\n",
        "        posD={}\n",
        "        j=0\n",
        "        for tag in pos_tagged.split(\" \"):\n",
        "            m=re.match(\"^([^\\/]+)\\/(.+)\", tag)\n",
        "            if m:\n",
        "                tag=re.sub(\"^\\_\",\"\",m.group(2)) # _noun\n",
        "                posD[(j,line[j])]=tag.lower() # to repair tokenization errors by LLM\n",
        "                j+=1\n",
        "\n",
        "        output=\"\"\n",
        "        for time in range(0, len(line)-1):\n",
        "            word=line[time]\n",
        "            sentence+=\" \"+word\n",
        "            output+=\" \"+word\n",
        "            probs=lmm.get_next_word_probabilities(sentence, top_k)\n",
        "\n",
        "            list_probs=list(probs)\n",
        "\n",
        "            #probsD=dict(list(probs)[:100])\n",
        "\n",
        "            # by means of deep copy...\n",
        "            probsD=dict(list_probs[:100])\n",
        "\n",
        "            #log probs to file:\n",
        "            #print(sentence)\n",
        "            #print(\"=========================================\\n\\n\")\n",
        "            #print(probsD,\"\\n\")\n",
        "\n",
        "            next_word=line[time+1]\n",
        "\n",
        "            for det in DETERMINERS:\n",
        "                if det not in probsD:\n",
        "                    probsD[det]=0.0\n",
        "                if next_word not in probsD:\n",
        "                    probsD[next_word]=0.0\n",
        "\n",
        "\n",
        "            probs_bigram=lmm.get_next_word_probabilities(word+\" \"+next_word, top_k) # the Leiden\n",
        "            list_probs_bigram=list(probs_bigram)\n",
        "            probsD_bigram=dict(list_probs_bigram[:100])\n",
        "            if next_word not in probsD_bigram:\n",
        "                probsD_bigram[next_word]=1e-5\n",
        "\n",
        "\n",
        "\n",
        "            # git clone https://github.com/huggingface/evaluate.git\n",
        "            #perplexity = evaluate.load(\"evaluate/metrics/perplexity/perplexity.py\")\n",
        "            perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
        "\n",
        "\n",
        "            missing=False\n",
        "            for det in DETERMINERS:\n",
        "                if probsD[det] > probsD[next_word] and posD[(time+1,next_word)] in ['noun', 'adjective']: #  trigger: pattern\n",
        "                    text1=' '.join(line[time:])\n",
        "                    text2 = line[time] + \" the \"+' '.join(line[time+1:]) # insert det\n",
        "\n",
        "                    results = perplexity.compute(model_id='gpt2',\n",
        "                             add_start_token=False,\n",
        "                             predictions=[text1,text2])\n",
        "                    ppl1=round(results[\"perplexities\"][0], 2)\n",
        "                    ppl2=round(results[\"perplexities\"][1], 2)\n",
        "                    #print(\"M:\",text1, ppl1, text2, ppl2)\n",
        "\n",
        "                    if ppl1>ppl2:\n",
        "                        output+=\" [MISSING DET:%s] \"%(det)\n",
        "                        print(\"MISSING DET \", next_word, det)\n",
        "                        missing=True\n",
        "            if not missing: # outdent to prevent 2x missing det\n",
        "                  if posD[(time,word)] in DETERMINER_TAGS and posD[(time+1, next_word)]!='noun': # trigger: pattern\n",
        "                      text1 = ' '.join(line[time-1:])\n",
        "                      text2 = line[time-1]+\" \"+' '.join(line[time+1:]) # remove det\n",
        "\n",
        "                      results = perplexity.compute(model_id='gpt2',\n",
        "                             add_start_token=False,\n",
        "                             predictions=[text1,text2])\n",
        "                      ppl1=round(results[\"perplexities\"][0], 2)\n",
        "                      ppl2=round(results[\"perplexities\"][1], 2)\n",
        "                      #print(\"E:\",text1, ppl1, text2, ppl2)\n",
        "\n",
        "                      if ppl1>ppl2:\n",
        "                          output+=\" [EXTRA DET:%s] \"%(word)\n",
        "                          print(\"EXTRA DET \", next_word, det)\n",
        "\n",
        "\n",
        "        sentence+=\" \"+line[time+1]\n",
        "        output+=\" \"+line[time+1]\n",
        "        Sentences.append(sentence)\n",
        "        Outputs.append(output)\n",
        "        Probs.append(probs)\n",
        "\n",
        "    outp=open(output_file,\"w\")\n",
        "\n",
        "    for (sentence, output) in zip(Sentences, Outputs):\n",
        "        outp.write(\"Sentence:%s\\n\"%(sentence))\n",
        "        outp.write(\"Output:%s\\n\"%(output))\n",
        "        outp.write(\"\\n\\n\")\n",
        "\n",
        "    outp.close()\n",
        "    print(\"See %s for output.\"%(output_file))\n",
        "    return Outputs\n"
      ],
      "metadata": {
        "id": "affZIL8xgZCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(input_file, top_k, output_file):\n",
        "    with open(input_file,\"r\") as f:\n",
        "        lines = [z for z in [x.rstrip().split(\" \") for x in f.readlines()]]\n",
        "\n",
        "    llm = LMHeadModel(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
        "    #lmm = LMHeadModel(\"NousResearch/Llama-2-7b-hf\") # No Huggingface key necessary for this one\n",
        "\n",
        "    outputs=generate_probs(lines, llm, top_k, output_file)\n",
        "    for output in outputs:\n",
        "        print(output)\n",
        "\n",
        "\n",
        "#if __name__==\"__main__\":\n",
        "#    if len(sys.argv)!=4:\n",
        "#        print(\"Usage: python get-nextword-probs.py <sentence file: one sentence per line> <desired top k words probabilities (number)> <output file name>\")\n",
        "#        print(\"Example: python3.9 get-nextword-probs.py sentences.txt 10 nextword-probs.txt\")\n",
        "#        exit(0)\n",
        "#    main(sys.argv[1], int(sys.argv[2]), sys.argv[3]) # file with one sentence per line, top k word probabilities (number), output file name\n",
        "    # Like: python python3.9 get-nextword-probs-determiners.py det.txt 10000 det.out\n"
      ],
      "metadata": {
        "id": "_72hEq7qgkX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put input file (1 sentence per line) in Drive, e.g. in temp folder (mount drive first, then use \"Upload to session storage\")\n",
        "main(\"det.txt\", 10000, \"det.out\")"
      ],
      "metadata": {
        "id": "yTGoqYYWhv-n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}