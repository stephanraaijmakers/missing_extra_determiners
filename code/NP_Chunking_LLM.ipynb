{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NP-CHUNKING (I-O-B tagging) with LLMs.  \n",
        "\n",
        "Stephan Raaijmakers, LUCL, 28.03.2025"
      ],
      "metadata": {
        "id": "O8PDeeLXrBFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3bor8eTJhFWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface"
      ],
      "metadata": {
        "id": "LRSPUzkkhLbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kgNt2X56qGEJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM , AutoTokenizer, pipeline\n",
        "from huggingface_hub import InferenceClient\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        ")"
      ],
      "metadata": {
        "id": "CQkjE1kMSwM7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface.llms import HuggingFacePipeline"
      ],
      "metadata": {
        "id": "AW6ixnlvUSpu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "_FZMirbBijx4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HUGGINGFACE_TOKEN=userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "Usl4J19uqqQ9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice you will need a Huggingface PRO account ($9/mo) for the InferenceClient on non-free tier models."
      ],
      "metadata": {
        "id": "RzcFGjsP-_nM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_iob_tags_ic(lines, output_file):\n",
        "    print(\"Generating IOB tags...\")\n",
        "    #HUGGINGFACE_TOKEN=os.environ[\"HUGGINGFACE_TOKEN\"] # run first: export HUGGINGFACE_TOKEN=\"...\" in shell\n",
        "    outp=open(output_file,\"w\")\n",
        "\n",
        "    client = InferenceClient(\n",
        "                api_key=HUGGINGFACE_TOKEN,\n",
        "            )\n",
        "    for i in tqdm(range(len(lines))):\n",
        "        sentence=' '.join(lines[i])\n",
        "\n",
        "        messages = [\n",
        "\t                {\n",
        "\t\t            \"role\": \"user\",\n",
        "\t\t            \"content\": \"Assign IOB tags for NP-chunking to the following sentence. Check that no taggings are repeated in your output. Every word in the sentence should have just one tag. If you have multiple options just output one. Tags are limited to O, B-NP, I-NP. Output format is: word/tag, per separate word. Example: The sandwich was good. Output: The/B-NP sandwich/I-NP was/O good/O. Sentence: \"+sentence\n",
        "\t                }\n",
        "                    ]\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "                 #model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "                 model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "\t             messages=messages,\n",
        "\t             max_tokens=500,\n",
        "                 temperature=0.0,\n",
        "            )\n",
        "\n",
        "        iob_tagged=completion.choices[0].message.content\n",
        "        iob_tagged=re.sub(\"\\n\",\" \",iob_tagged)\n",
        "        iob_tagged=re.sub(\"Here is the output: \",\"\",iob_tagged)\n",
        "        outp.write(\"Sentence:%s\\nTags:%s\\n\"%(sentence,iob_tagged))"
      ],
      "metadata": {
        "id": "j_-qDpYgqN5O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceEndpoint(\n",
        "    #repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03,\n",
        "    temperature=0.0\n",
        ")"
      ],
      "metadata": {
        "id": "ErD3Dolgl8WY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_iob_tags_hf(lines, output_file):\n",
        "    print(\"Generating IOB tags...\")\n",
        "    #HUGGINGFACE_TOKEN=os.environ[\"HUGGINGFACE_TOKEN\"] # run first: export HUGGINGFACE_TOKEN=\"...\" in shell\n",
        "    outp=open(output_file,\"w\")\n",
        "\n",
        "    chat_model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "    for i in tqdm(range(len(lines))):\n",
        "        sentence=' '.join(lines[i])\n",
        "\n",
        "        messages = [\n",
        "        SystemMessage(content=\"Assign IOB tags for NP-chunking to the following sentence. Check that no taggings are repeated in your output. Every word in the sentence should have just one tag. If you have multiple options just output one. Tags are limited to O, B-NP, I-NP. Output format is: word/tag, per separate word. Example: The sandwich was good. Output: The/B-NP sandwich/I-NP was/O good/O\"),\n",
        "        HumanMessage(\n",
        "        content=\"Sentence:\"+sentence\n",
        "          ),\n",
        "        ]\n",
        "\n",
        "        completion = chat_model.invoke(messages)\n",
        "\n",
        "        iob_tagged=completion.content\n",
        "        iob_tagged=re.sub(\"\\n\",\" \",iob_tagged)\n",
        "        iob_tagged=re.sub(\"Here is the output: \",\"\",iob_tagged)\n",
        "        outp.write(\"Sentence:%s\\nTags:%s\\n\"%(sentence,iob_tagged))"
      ],
      "metadata": {
        "id": "BVvZsQ4SScyw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main(input_file, output_file):\n",
        "    with open(input_file,\"r\") as f:\n",
        "        lines = [z for z in [x.rstrip().split(\" \") for x in f.readlines()]]\n",
        "    generate_iob_tags_hf(lines, output_file)\n",
        "    #generate_iob_tags_ic(lines, output_file)\n",
        "    print(\"See \", output_file)"
      ],
      "metadata": {
        "id": "6emfwDt9qTTH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(\"det.txt\", \"iob.out\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMuejpxcqZzl",
        "outputId": "25ce1677-3367-4c8e-c447-153f87f1c6da"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating IOB tags...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:01<00:00,  7.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "See  iob.out\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}